<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>climate visual transformer</title><!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="climate visual transformer" />
<meta name="author" content="Cameron Kaminski" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="For my final project in my CSCI481 (Deep Learning) course at Western Washington University: I aimed to localize weather patterns in time using a grid map of daily-mean temperatures across the globe for every day in the year." />
<meta property="og:description" content="For my final project in my CSCI481 (Deep Learning) course at Western Washington University: I aimed to localize weather patterns in time using a grid map of daily-mean temperatures across the globe for every day in the year." />
<link rel="canonical" href="http://localhost:4000/machine-learning/2023/06/09/climate-vit.html" />
<meta property="og:url" content="http://localhost:4000/machine-learning/2023/06/09/climate-vit.html" />
<meta property="og:site_name" content="cameron g. kaminski" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-06-09T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="climate visual transformer" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Cameron Kaminski"},"dateModified":"2023-06-09T00:00:00-07:00","datePublished":"2023-06-09T00:00:00-07:00","description":"For my final project in my CSCI481 (Deep Learning) course at Western Washington University: I aimed to localize weather patterns in time using a grid map of daily-mean temperatures across the globe for every day in the year.","headline":"climate visual transformer","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine-learning/2023/06/09/climate-vit.html"},"url":"http://localhost:4000/machine-learning/2023/06/09/climate-vit.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="cameron g. kaminski" /><link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
</head><body a="dark">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/">..</a><article>
  <p class="post-meta">
    <time datetime="2023-06-09 00:00:00 -0700">2023-06-09</time>
  </p>
  
  <h1>climate visual transformer</h1>

  <p>For my final project in my CSCI481 (Deep Learning) course at Western Washington
University: I aimed to localize weather patterns in time using a grid map of 
daily-mean temperatures across the globe for every day in the year.</p>

<p>The given dataset consisted of global temperature maps with daily-mean 
temperatures (measured in Kelvin) across a years time, of which there were 659
samples (note: for anonymity the exact dataset was kept secret). Each datapoint
was of the size [128, 256] correlating to [latitude/height, longitude/width].
Finally, the task was to predict the starting date, disregarding year, of a 
given 7-day sequence of data.</p>

<p><img src="/assets/img/vit_map.jpg" alt="Image description" class="ioda" /></p>

<p>For the model I chose to attempt solving the task while using my own 
architectured ViT. At the time of this project visual transformers were still
new technology and not yet built into popular machine learning frameworks such
as Pytorch and Tensorflow. This led me to architect my own ViT model based on
the source code available from “An Image is Worth 16x16 Words: Transformers for
Image Recognition at Scale”, Dosovitskiy et al. (2020). However, considerable 
alteration had to be made to support a 10gb v-ram limitation and to account for 
temporal dependencies in the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s"> ViT implementation; note this is a very oversimplified version which
        should be fine for the Climate Dataset.
        :param img_size: size of the image (height x weight)
        :param patch_size: size of the patch (height x weight)
        :param input_channels: number of channels, should be 1
        :param num_classes: number of classes, should be 358
        :param embed_dim: dimensionality of the token/patch embeddings
        :param depth: number of blocks
        :param num_heads: number of attention heads
        :param hidden_layer_ratio: determines the hidden dim for the MLP
        :param qkv_bias: includes bias to the qkv
        :param p: dropout probability
        :param attn_p: dropout probability
        </span><span class="sh">"""</span>
    
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
                <span class="n">self</span><span class="p">,</span>
                <span class="n">img_size</span><span class="p">:</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
                <span class="n">patch_size</span><span class="p">:</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">358</span><span class="p">,</span>
                <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
                <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                <span class="n">hidden_layer_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span>
                <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">p</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                <span class="n">attn_p</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    
            <span class="n">self</span><span class="p">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="nc">PatchEmbed</span><span class="p">(</span>
                <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span>
                <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                <span class="n">num_frames</span><span class="o">=</span><span class="n">num_frames</span><span class="p">,</span>
                <span class="n">input_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
                <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">patch_embed</span><span class="p">.</span><span class="n">n_patches</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pos_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    
            <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="nc">Block</span><span class="p">(</span>
                        <span class="n">dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
                        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                        <span class="n">hidden_layer_ratio</span><span class="o">=</span><span class="n">hidden_layer_ratio</span><span class="p">,</span>
                        <span class="n">qkv_bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">,</span>
                        <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
                        <span class="n">attn_p</span><span class="o">=</span><span class="n">attn_p</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
    
            <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="sh">"""</span><span class="s"> Run the forward pass for the transformer.
            :param x: (batch_size, num_frames, input_channels, img_height, img_width)
            :return x: (batch_size)
            </span><span class="sh">"""</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, num_frames, num_patches, embed_dim)
</span>    
            <span class="n">cls_token</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cls_token</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_frames</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (batch_size, num_frames, 1, embed_dim)
</span>    
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_frames, 1 + n_patches, embed_dim)
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pos_embed</span>  <span class="c1"># (batch_size, num_frames, 1 + n_patches, embed_dim)
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_frames</span><span class="p">):</span>
                <span class="n">input_frame</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">frame</span><span class="p">,</span> <span class="p">:]</span>
    
                <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">:</span>
                    <span class="n">input_frame</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">input_frame</span><span class="p">)</span>
    
                <span class="n">output_frame</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">input_frame</span><span class="p">)</span>
                <span class="n">cls_token_frame</span> <span class="o">=</span> <span class="n">output_frame</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">output_frame</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">cls_token_frame</span><span class="p">)</span>
                <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output_frame</span><span class="p">)</span>
    
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>A fascinating benefit of the ViT model was we could easily convert it to predict
for either classification or regression. Classification required taking each day 
(0…364) as its own class. Regression was a bit more tricky. To handle the 
cyclical nature of data I mapped the year around the unit circle, such that each 
day correlated to every 1/365th point. Then the model would predict the 
Cartesian (x,y) coordinates for every feature.</p>

<p>Using limited resources, i.e. Nvidia 2080 GPUs I was able to achieve the
following performance.</p>

<p><img src="/assets/img/vit_table.jpeg" alt="Image description" class="ioda" /></p>

<p>Considering the heavy computation limitation, and the natural tendencies for ViT
models to become significantly more performant as their complexity increases,
I suspect that given more computational power we would see significantly
better performance.</p>


</article>
      </div>
    </main>
  </body>
</html>